{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69aa71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import re\n",
    "from pandas import Series\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('FinalWNVData v.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27958aed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10506 entries, 0 to 10505\n",
      "Data columns (total 69 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Unnamed: 0              10506 non-null  int64  \n",
      " 1   Date                    10506 non-null  object \n",
      " 2   Address                 10506 non-null  object \n",
      " 3   Species                 10506 non-null  object \n",
      " 4   Block                   10506 non-null  int64  \n",
      " 5   Street                  10506 non-null  object \n",
      " 6   Trap                    10506 non-null  object \n",
      " 7   AddressNumberAndStreet  10506 non-null  object \n",
      " 8   Latitude                10506 non-null  float64\n",
      " 9   Longitude               10506 non-null  float64\n",
      " 10  AddressAccuracy         10506 non-null  int64  \n",
      " 11  NumMosquitos            10506 non-null  int64  \n",
      " 12  WnvPresent              10506 non-null  int64  \n",
      " 13  Tmax                    10506 non-null  float64\n",
      " 14  Tmin                    10506 non-null  float64\n",
      " 15  Tavg                    10506 non-null  float64\n",
      " 16  Depart                  10506 non-null  float64\n",
      " 17  DewPoint                10506 non-null  float64\n",
      " 18  WetBulb                 10413 non-null  float64\n",
      " 19  Heat                    10506 non-null  float64\n",
      " 20  Cool                    10506 non-null  float64\n",
      " 21  Sunrise                 10506 non-null  object \n",
      " 22  Sunset                  10506 non-null  object \n",
      " 23  CodeSum                 10506 non-null  object \n",
      " 24  SnowFall                10506 non-null  float64\n",
      " 25  PrecipTotal             10506 non-null  float64\n",
      " 26  StnPressure             10413 non-null  float64\n",
      " 27  SeaLevel                10506 non-null  float64\n",
      " 28  ResultSpeed             10506 non-null  float64\n",
      " 29  ResultDir               10506 non-null  int64  \n",
      " 30  AvgSpeed                10506 non-null  float64\n",
      " 31  BR                      10506 non-null  int64  \n",
      " 32  DZ                      10506 non-null  int64  \n",
      " 33  FG                      10506 non-null  int64  \n",
      " 34  HZ                      10506 non-null  int64  \n",
      " 35  RA                      10506 non-null  int64  \n",
      " 36  TS                      10506 non-null  int64  \n",
      " 37  TSRA                    10506 non-null  int64  \n",
      " 38  VCTS                    10506 non-null  int64  \n",
      " 39  Time                    113 non-null    object \n",
      " 40  Friday                  10506 non-null  int64  \n",
      " 41  Monday                  10506 non-null  int64  \n",
      " 42  Thursday                10506 non-null  int64  \n",
      " 43  Tuesday                 10506 non-null  int64  \n",
      " 44  Wednesday               10506 non-null  int64  \n",
      " 45  August                  10506 non-null  int64  \n",
      " 46  July                    10506 non-null  int64  \n",
      " 47  June                    10506 non-null  int64  \n",
      " 48  May                     10506 non-null  int64  \n",
      " 49  October                 10506 non-null  int64  \n",
      " 50  September               10506 non-null  int64  \n",
      " 51  TimeSinceLastBR         10249 non-null  float64\n",
      " 52  TimeSinceLastDZ         4001 non-null   float64\n",
      " 53  TimeSinceLastFG         4440 non-null   float64\n",
      " 54  TimeSinceLastHZ         9856 non-null   float64\n",
      " 55  TimeSinceLastRA         10249 non-null  float64\n",
      " 56  TimeSinceLastTS         9401 non-null   float64\n",
      " 57  TimeSinceLastTSRA       9940 non-null   float64\n",
      " 58  TimeSinceLastVCTS       5605 non-null   float64\n",
      " 59  Hyde Park Township      10506 non-null  int64  \n",
      " 60  Jefferson Township      10506 non-null  int64  \n",
      " 61  Lake Township           10506 non-null  int64  \n",
      " 62  Lake View Township      10506 non-null  int64  \n",
      " 63  Leyden Township         10506 non-null  int64  \n",
      " 64  Norwood Park Township   10506 non-null  int64  \n",
      " 65  Rogers Park Township    10506 non-null  int64  \n",
      " 66  Stickney Township       10506 non-null  int64  \n",
      " 67  Thornton Township       10506 non-null  int64  \n",
      " 68  Worth Township          10506 non-null  int64  \n",
      "dtypes: float64(24), int64(35), object(10)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cfced",
   "metadata": {},
   "source": [
    "# Weight of Evidence/Information Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e90fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's split our data into training and testing sets.\n",
    "X = df.drop(columns=['WnvPresent'])\n",
    "y = df['WnvPresent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31883833",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    #print(\"justmiss\", justmiss)\n",
    "    #print(\"notmiss\", notmiss)\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            #print(\"I am here 1\",r, n,len(d2))\n",
    "            n = n - 1 \n",
    "            \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "            #print(\"I am here e\",n)\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        #print(\"I am second step \",r, n)\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        #print(justmiss.count().Y)\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    print(np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT))\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    #print(\"hi\",d3.IV )\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    for i in x:\n",
    "        print(i)\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                #print(\"Number and unique value greater than 2\")\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                #print(\"I am here 2\")\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n"
     ]
    }
   ],
   "source": [
    "final_iv, IV = data_vars(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
